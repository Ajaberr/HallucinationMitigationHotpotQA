{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "YiRXgSXAiCAOcg9ae33vLQWS",
      "metadata": {
        "executionInfo": {
          "elapsed": 297,
          "status": "ok",
          "timestamp": 1763340592644,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "YiRXgSXAiCAOcg9ae33vLQWS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: Qwen2.5-7B-Instruct requires significant VRAM (>= 16GB) to run.\n",
        "# Consider using quantization (load_in_4bit=True) or a smaller model if resources are limited.\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DATASET_NAME = \"hotpot_qa\"\n",
        "SUBSET_NAME = \"fullwiki\"\n",
        "SPLIT = \"validation\" # Use the validation set for testing\n",
        "NUM_SAMPLES = 100    # Set a small number for quick testing (e.g., 100). Increase for full evaluation.\n",
        "\n",
        "def load_model_and_tokenizer(model_id):\n",
        "    \"\"\"Loads the model and tokenizer, optimizing for available hardware.\"\"\"\n",
        "    print(f\"Loading model: {model_id}...\")\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Setup for model loading (using bfloat16 for better performance on modern GPUs)\n",
        "    kwargs = {\n",
        "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 8 else torch.float16,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"low_cpu_mem_usage\": True\n",
        "    }\n",
        "\n",
        "    # Try using 4-bit quantization if GPU is available to save VRAM\n",
        "    if device == \"cuda\" and torch.cuda.get_device_properties(0).total_memory / (1024**3) < 24:\n",
        "        print(\"Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\")\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            kwargs[\"quantization_config\"] = bnb_config\n",
        "        except ImportError:\n",
        "            print(\"Install 'bitsandbytes' for 4-bit quantization to work on low VRAM.\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded successfully on device: {device}\")\n",
        "        return tokenizer, model, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Please ensure you have sufficient hardware (GPU/RAM) and required libraries installed.\")\n",
        "        return None, None, None\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lowercases, removes punctuation and articles (a, an, the), and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        # Create a translation table to remove punctuation\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        return text.translate(translator)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    \"\"\"Returns a list of tokens after normalization.\"\"\"\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    \"\"\"\n",
        "    Computes F1 score based on token overlap.\n",
        "    NOTE: Returns a single float value for the F1 score.\n",
        "    \"\"\"\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "\n",
        "    # Handle edge case where one or both answers are empty\n",
        "    if not gold_toks or not pred_toks:\n",
        "        # F1 is 1.0 if both are empty (perfect match), 0.0 otherwise.\n",
        "        return float(gold_toks == pred_toks)\n",
        "\n",
        "    common = 0\n",
        "    pred_counter = Counter(pred_toks)\n",
        "\n",
        "    for gold_tok in gold_toks:\n",
        "        if pred_counter[gold_tok] > 0:\n",
        "            common += 1\n",
        "            pred_counter[gold_tok] -= 1\n",
        "\n",
        "    prec = common / len(pred_toks)\n",
        "    rec = common / len(gold_toks)\n",
        "\n",
        "    f1 = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "def compute_metrics(gold_answers, pred_answers):\n",
        "    \"\"\"Calculates Exact Match (EM) and F1 Score for a list of answers.\"\"\"\n",
        "    em_total = 0\n",
        "    f1_total = 0\n",
        "\n",
        "    for gold, pred in zip(gold_answers, pred_answers):\n",
        "        # HotpotQA has multiple possible gold answers; use the best one\n",
        "        # For simplicity, we just use the first gold answer text.\n",
        "        gold_text = gold[0]\n",
        "        pred_text = pred\n",
        "\n",
        "        # Exact Match Check\n",
        "        norm_gold = normalize_answer(gold_text)\n",
        "        norm_pred = normalize_answer(pred_text)\n",
        "        em = 1.0 if norm_gold == norm_pred else 0.0\n",
        "\n",
        "        # F1 Check\n",
        "        f1 = compute_f1(gold_text, pred_text)\n",
        "\n",
        "        em_total += em\n",
        "        f1_total += f1\n",
        "\n",
        "    num_samples = len(gold_answers)\n",
        "    return {\n",
        "        \"EM\": (em_total / num_samples) * 100,\n",
        "        \"F1\": (f1_total / num_samples) * 100\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the HotpotQA evaluation.\"\"\"\n",
        "    tokenizer, model, device = load_model_and_tokenizer(MODEL_ID)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # 1. Load Dataset\n",
        "    print(f\"\\nLoading HotpotQA dataset (split: {SPLIT}, samples: {NUM_SAMPLES})...\")\n",
        "    try:\n",
        "        dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split=SPLIT)\n",
        "        # Filter for the first N samples\n",
        "        dataset = dataset.select(range(min(NUM_SAMPLES, len(dataset))))\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    gold_answers = []\n",
        "    pred_answers = []\n",
        "\n",
        "    print(\"\\nStarting closed-book evaluation...\")\n",
        "\n",
        "    for i, example in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "        # HotpotQA question format\n",
        "        question = example['question']\n",
        "\n",
        "        # The gold answer is a list of possible answers\n",
        "        # HotpotQA stores the answer as a string or list of strings.\n",
        "        # We ensure it's a list for consistency with multiple possible answers.\n",
        "        gold_answer_list = example['answer'] if isinstance(example['answer'], list) else [example['answer']]\n",
        "        gold_answers.append(gold_answer_list)\n",
        "\n",
        "        # --- Closed-Book Prompting ---\n",
        "        # The prompt only contains the question, not the context,\n",
        "        # forcing the model to rely on its internal knowledge.\n",
        "        prompt = f\"\"\"You are an expert at giving concise answers. Do not give any explanations, only a short answer. For example:\n",
        "        Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
        "        Answer: Arthur's Magazine\n",
        "\n",
        "        Question: Is Children's National Medical Center or MedStar Washington Hospital Center the largest private hospital in Washington, D.C.?\n",
        "        Answer: MedStar Washington Hospital Center\n",
        "\n",
        "        Now answer the question:\n",
        "\n",
        "        Question: {question}\n",
        "        Answer: \"\"\"\n",
        "\n",
        "        # Qwen-specific instruction format (if using the instruct version)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50, # Sufficient for a concise QA answer\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=tokenizer.eos_token_id # Important for generation\n",
        "            )\n",
        "\n",
        "        # Decode the generated text, skipping the input prompt\n",
        "        generated_text = tokenizer.decode(output[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        pred_answers.append(generated_text)\n",
        "\n",
        "        if i % (NUM_SAMPLES // 5 or 1) == 0 and i > 0:\n",
        "            print(f\"\\n--- Sample {i+1}/{len(dataset)} ---\")\n",
        "            print(f\"Q: {question}\")\n",
        "            # Print the first gold answer for readability\n",
        "            print(f\"Gold: {gold_answer_list[0]}\")\n",
        "            print(f\"Pred: {generated_text}\")\n",
        "\n",
        "    # 3. Compute Metrics\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "    metrics = compute_metrics(gold_answers, pred_answers)\n",
        "\n",
        "    print(f\"Results for {MODEL_ID} on HotpotQA ({len(dataset)} samples):\")\n",
        "    print(f\"  Exact Match (EM): {metrics['EM']:.2f}%\")\n",
        "    print(f\"  F1 Score (F1): {metrics['F1']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "yLnst4blh6uo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676,
          "referenced_widgets": [
            "3d8336c775eb4a79975ccced2ca0032d",
            "f723cddcf74942c6b0bf8fc94bfaee6b",
            "7577706f2fa0486f983f049ad5de5531",
            "6fa76262f91a4078852787d48bbd003c",
            "8e04976f083042d9b2b736abab09b75c",
            "a3f9b1b251f24884ada2abbf3700e965",
            "c9ef8074f47a4772b695803a7170cd0b",
            "5838ddd4ba514b16928be8fb51568029",
            "260e11fef943412d8d3656a5716f897d",
            "46200cb09c874a35a1f3e4c769d86c89",
            "b42ffb2230bb408a957f39d9a8ad1d1a"
          ]
        },
        "executionInfo": {
          "elapsed": 128195,
          "status": "ok",
          "timestamp": 1763340721389,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "yLnst4blh6uo",
        "outputId": "ce100031-dff2-469f-8acf-c59b31cbb3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-7B-Instruct...\n",
            "Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d8336c775eb4a79975ccced2ca0032d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully on device: cuda\n",
            "\n",
            "Loading HotpotQA dataset (split: validation, samples: 100)...\n",
            "Dataset loaded with 100 samples.\n",
            "\n",
            "Starting closed-book evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 21/100 [00:23<02:19,  1.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 21/100 ---\n",
            "Q: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?\n",
            "Gold: Pedro Rodríguez\n",
            "Pred: Answer: As of the latest information available, no other Mexican Formula One race car driver has consistently held the podium besides Sergio Perez, who was born in 1990 and drives for Force India (now known as Aston Martin). However, this\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [00:39<00:46,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 41/100 ---\n",
            "Q: Which dog's ancestors include Gordon and Irish Setters: the Manchester Terrier or the Scotch Collie?\n",
            "Gold: Scotch Collie\n",
            "Pred: Scotch Collie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [01:04<01:00,  1.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 61/100 ---\n",
            "Q: What distinction is held by the former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting?\n",
            "Gold: shortest player ever to play in the National Basketball Association\n",
            "Pred: Former NBA player Larry Brown holds that distinction.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 81/100 [01:28<00:16,  1.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 81/100 ---\n",
            "Q: What is the county seat of the county where East Lempster, New Hampshire is located?\n",
            "Gold: Newport\n",
            "Pred: Lempster\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:50<00:00,  1.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Complete ---\n",
            "Results for Qwen/Qwen2.5-7B-Instruct on HotpotQA (100 samples):\n",
            "  Exact Match (EM): 8.00%\n",
            "  F1 Score (F1): 17.95%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "-dyxiiVNbD9p",
      "metadata": {
        "executionInfo": {
          "elapsed": 5807,
          "status": "ok",
          "timestamp": 1763342849164,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "-dyxiiVNbD9p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: Qwen2.5-7B-Instruct requires significant VRAM (>= 16GB) to run.\n",
        "# Consider using quantization (load_in_4bit=True) or a smaller model if resources are limited.\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DATASET_NAME = \"hotpot_qa\"\n",
        "SUBSET_NAME = \"fullwiki\"\n",
        "SPLIT = \"validation\" # Use the validation set for testing\n",
        "NUM_SAMPLES = 100    # Set a small number for quick testing (e.g., 100). Increase for full evaluation.\n",
        "\n",
        "def load_model_and_tokenizer(model_id):\n",
        "    \"\"\"Loads the model and tokenizer, optimizing for available hardware.\"\"\"\n",
        "    print(f\"Loading model: {model_id}...\")\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Setup for model loading (using bfloat16 for better performance on modern GPUs)\n",
        "    kwargs = {\n",
        "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 8 else torch.float16,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"low_cpu_mem_usage\": True\n",
        "    }\n",
        "\n",
        "    # Try using 4-bit quantization if GPU is available to save VRAM\n",
        "    if device == \"cuda\" and torch.cuda.get_device_properties(0).total_memory / (1024**3) < 24:\n",
        "        print(\"Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\")\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            kwargs[\"quantization_config\"] = bnb_config\n",
        "        except ImportError:\n",
        "            print(\"Install 'bitsandbytes' for 4-bit quantization to work on low VRAM.\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded successfully on device: {device}\")\n",
        "        return tokenizer, model, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Please ensure you have sufficient hardware (GPU/RAM) and required libraries installed.\")\n",
        "        return None, None, None\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lowercases, removes punctuation and articles (a, an, the), and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        # Create a translation table to remove punctuation\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        return text.translate(translator)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    \"\"\"Returns a list of tokens after normalization.\"\"\"\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    \"\"\"\n",
        "    Computes F1 score based on token overlap.\n",
        "    NOTE: Returns a single float value for the F1 score.\n",
        "    \"\"\"\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "\n",
        "    # Handle edge case where one or both answers are empty\n",
        "    if not gold_toks or not pred_toks:\n",
        "        # F1 is 1.0 if both are empty (perfect match), 0.0 otherwise.\n",
        "        return float(gold_toks == pred_toks)\n",
        "\n",
        "    common = 0\n",
        "    pred_counter = Counter(pred_toks)\n",
        "\n",
        "    for gold_tok in gold_toks:\n",
        "        if pred_counter[gold_tok] > 0:\n",
        "            common += 1\n",
        "            pred_counter[gold_tok] -= 1\n",
        "\n",
        "    prec = common / len(pred_toks)\n",
        "    rec = common / len(gold_toks)\n",
        "\n",
        "    f1 = (2 * prec * rec) / (prec + rec) if (prec + rec) > 0 else 0\n",
        "    return f1\n",
        "\n",
        "def compute_metrics(gold_answers, pred_answers):\n",
        "    \"\"\"Calculates Exact Match (EM) and F1 Score for a list of answers.\"\"\"\n",
        "    em_total = 0\n",
        "    f1_total = 0\n",
        "\n",
        "    for gold, pred in zip(gold_answers, pred_answers):\n",
        "        # HotpotQA has multiple possible gold answers; use the best one\n",
        "        # For simplicity, we just use the first gold answer text.\n",
        "        gold_text = gold[0]\n",
        "        pred_text = pred\n",
        "\n",
        "        # Exact Match Check\n",
        "        norm_gold = normalize_answer(gold_text)\n",
        "        norm_pred = normalize_answer(pred_text)\n",
        "        em = 1.0 if norm_gold == norm_pred else 0.0\n",
        "\n",
        "        # F1 Check\n",
        "        f1 = compute_f1(gold_text, pred_text)\n",
        "\n",
        "        em_total += em\n",
        "        f1_total += f1\n",
        "\n",
        "    num_samples = len(gold_answers)\n",
        "    return {\n",
        "        \"EM\": (em_total / num_samples) * 100,\n",
        "        \"F1\": (f1_total / num_samples) * 100\n",
        "    }\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the HotpotQA evaluation.\"\"\"\n",
        "    tokenizer, model, device = load_model_and_tokenizer(MODEL_ID)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # 1. Load Dataset\n",
        "    print(f\"\\nLoading HotpotQA dataset (split: {SPLIT}, samples: {NUM_SAMPLES})...\")\n",
        "    try:\n",
        "        dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split=SPLIT)\n",
        "        # Filter for the first N samples\n",
        "        dataset = dataset.select(range(min(NUM_SAMPLES, len(dataset))))\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    gold_answers = []\n",
        "    pred_answers = []\n",
        "\n",
        "    print(\"\\nStarting closed-book evaluation...\")\n",
        "\n",
        "    for i, example in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "        # HotpotQA question format\n",
        "        question = example['question']\n",
        "\n",
        "        # The gold answer is a list of possible answers\n",
        "        # HotpotQA stores the answer as a string or list of strings.\n",
        "        # We ensure it's a list for consistency with multiple possible answers.\n",
        "        gold_answer_list = example['answer'] if isinstance(example['answer'], list) else [example['answer']]\n",
        "        gold_answers.append(gold_answer_list)\n",
        "\n",
        "        # --- Closed-Book Prompting ---\n",
        "        # The prompt only contains the question, not the context,\n",
        "        # forcing the model to rely on its internal knowledge.\n",
        "        prompt = f\"\"\"You are an expert at giving concise answers. Do not give any explanations, only a short answer. For example:\n",
        "        Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
        "        Answer: Arthur's Magazine\n",
        "\n",
        "        Question: Is Children's National Medical Center or MedStar Washington Hospital Center the largest private hospital in Washington, D.C.?\n",
        "        Answer: MedStar Washington Hospital Center\n",
        "\n",
        "        Now answer the question:\n",
        "\n",
        "        Question: {question}\n",
        "        Answer: \"\"\"\n",
        "\n",
        "        # Qwen-specific instruction format (if using the instruct version)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50, # Sufficient for a concise QA answer\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=tokenizer.eos_token_id # Important for generation\n",
        "            )\n",
        "\n",
        "        # Decode the generated text, skipping the input prompt\n",
        "        generated_text = tokenizer.decode(output[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        pred_answers.append(generated_text)\n",
        "\n",
        "        if i % (NUM_SAMPLES // 5 or 1) == 0 and i > 0:\n",
        "            print(f\"\\n--- Sample {i+1}/{len(dataset)} ---\")\n",
        "            print(f\"Q: {question}\")\n",
        "            # Print the first gold answer for readability\n",
        "            print(f\"Gold: {gold_answer_list[0]}\")\n",
        "            print(f\"Pred: {generated_text}\")\n",
        "\n",
        "    # 3. Compute Metrics\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "    metrics = compute_metrics(gold_answers, pred_answers)\n",
        "\n",
        "    print(f\"Results for {MODEL_ID} on HotpotQA ({len(dataset)} samples):\")\n",
        "    print(f\"  Exact Match (EM): {metrics['EM']:.2f}%\")\n",
        "    print(f\"  F1 Score (F1): {metrics['F1']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "AL6vS6PLbGT7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4258b384016b413492ef322ba4df4ca4",
            "b2d49d848e4d4d6ea9b0dbc4dfa6577a",
            "74355440b8b741d2942962c0e4b0d6f1",
            "325a2706c18945468fca290325026053",
            "44185fc3ee3b442aa4a958c54396638d",
            "f1c5e4cb8591429abe0581531ddd07f3",
            "d85739665e9d492ea22b8baa12853da8",
            "59d71ac2cba3451ab66caaa3ac250a99",
            "07a57c7c68d44b36bb04ad81b5f102b1",
            "2bfc86dd80f14d5b940f71e7c45c3e59",
            "c06096ac9fb14e19bbf98ece78e1bf9b"
          ]
        },
        "executionInfo": {
          "elapsed": 104218,
          "status": "ok",
          "timestamp": 1763342953380,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "AL6vS6PLbGT7",
        "outputId": "4e5ac03c-fb2b-42d3-9c78-6f447ee7f06a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-7B-Instruct...\n",
            "Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4258b384016b413492ef322ba4df4ca4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully on device: cuda\n",
            "\n",
            "Loading HotpotQA dataset (split: validation, samples: 100)...\n",
            "Dataset loaded with 100 samples.\n",
            "\n",
            "Starting closed-book evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 21%|██        | 21/100 [00:18<01:04,  1.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 21/100 ---\n",
            "Q: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?\n",
            "Gold: Pedro Rodríguez\n",
            "Pred: Stevens Davies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [00:31<00:40,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 41/100 ---\n",
            "Q: Which dog's ancestors include Gordon and Irish Setters: the Manchester Terrier or the Scotch Collie?\n",
            "Gold: Scotch Collie\n",
            "Pred: Scotch Collie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [00:46<00:31,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 61/100 ---\n",
            "Q: What distinction is held by the former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting?\n",
            "Gold: shortest player ever to play in the National Basketball Association\n",
            "Pred: first head coach of the Charlotte Sting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 81/100 [01:02<00:13,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 81/100 ---\n",
            "Q: What is the county seat of the county where East Lempster, New Hampshire is located?\n",
            "Gold: Newport\n",
            "Pred: Lempster\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:16<00:00,  1.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Complete ---\n",
            "Results for Qwen/Qwen2.5-7B-Instruct on HotpotQA (100 samples):\n",
            "  Exact Match (EM): 18.00%\n",
            "  F1 Score (F1): 29.46%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "rTsF7HINbHVs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712,
          "referenced_widgets": [
            "8a15fe4fb0074e6cac55be9d48680d33",
            "60234a049e054bcbba47e586b30fd17d",
            "fedf1a7d07004549bd5c53af7526cf86",
            "047df1f9c5c749bbaa6e0496dacc494a",
            "b0f86f94c2044b4597ad938d8d87bcd1",
            "5ce7124a030a4c768b392e4b10b6ec58",
            "8e31ca3154264784b3c9c2f9541dd691",
            "f1f0d9e0be4e459f8bcb73b7498cce11",
            "f7a4e0b529fc4ebb936a93407c8ffee9",
            "9e1e960e5ef748e1af307b8891a1ca68",
            "56ee725339e04140b2d8962ab9e211be"
          ]
        },
        "executionInfo": {
          "elapsed": 95073,
          "status": "ok",
          "timestamp": 1763344656552,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 300
        },
        "id": "rTsF7HINbHVs",
        "outputId": "8cc021c2-edcf-471a-d215-9491d7a2e441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-7B-Instruct...\n",
            "Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a15fe4fb0074e6cac55be9d48680d33",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully on device: cuda\n",
            "\n",
            "Loading HotpotQA dataset (split: validation, samples: 100)...\n",
            "Dataset loaded with 100 samples.\n",
            "\n",
            "Starting closed-book evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 21/100 [00:18<01:06,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 21/100 ---\n",
            "Q: Which other Mexican Formula One race car driver has held the podium besides the Force India driver born in 1990?\n",
            "Gold: Pedro Rodríguez\n",
            "Pred: Stevens Davies\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 41/100 [00:32<00:41,  1.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 41/100 ---\n",
            "Q: Which dog's ancestors include Gordon and Irish Setters: the Manchester Terrier or the Scotch Collie?\n",
            "Gold: Scotch Collie\n",
            "Pred: Scotch Collie\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 61/100 [00:47<00:31,  1.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 61/100 ---\n",
            "Q: What distinction is held by the former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting?\n",
            "Gold: shortest player ever to play in the National Basketball Association\n",
            "Pred: first head coach of the Charlotte Sting\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 81/100 [01:03<00:13,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 81/100 ---\n",
            "Q: What is the county seat of the county where East Lempster, New Hampshire is located?\n",
            "Gold: Newport\n",
            "Pred: Lempster\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:16<00:00,  1.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Complete ---\n",
            "Results for Qwen/Qwen2.5-7B-Instruct on HotpotQA (100 samples):\n",
            "  Exact Match (EM): 18.00%\n",
            "  F1 Score (F1): 29.46%\n",
            "  Precision: 30.83%\n",
            "  Recall: 29.38%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: Qwen2.5-7B-Instruct requires significant VRAM (>= 16GB) to run.\n",
        "# Consider using quantization (load_in_4bit=True) or a smaller model if resources are limited.\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DATASET_NAME = \"hotpot_qa\"\n",
        "SUBSET_NAME = \"fullwiki\"\n",
        "SPLIT = \"validation\" # Use the validation set for testing\n",
        "NUM_SAMPLES = 100    # Set a small number for quick testing (e.g., 100). Increase for full evaluation.\n",
        "\n",
        "def load_model_and_tokenizer(model_id):\n",
        "    \"\"\"Loads the model and tokenizer, optimizing for available hardware.\"\"\"\n",
        "    print(f\"Loading model: {model_id}...\")\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Setup for model loading (using bfloat16 for better performance on modern GPUs)\n",
        "    kwargs = {\n",
        "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 8 else torch.float16,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"low_cpu_mem_usage\": True\n",
        "    }\n",
        "\n",
        "    # Try using 4-bit quantization if GPU is available to save VRAM\n",
        "    if device == \"cuda\" and torch.cuda.get_device_properties(0).total_memory / (1024**3) < 24:\n",
        "        print(\"Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\")\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            kwargs[\"quantization_config\"] = bnb_config\n",
        "        except ImportError:\n",
        "            print(\"Install 'bitsandbytes' for 4-bit quantization to work on low VRAM.\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded successfully on device: {device}\")\n",
        "        return tokenizer, model, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Please ensure you have sufficient hardware (GPU/RAM) and required libraries installed.\")\n",
        "        return None, None, None\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Official HotpotQA normalization for answers.\n",
        "    Lowercases, removes punctuation, articles (a, an, the), and extra whitespace.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"Computes F1 score based on token overlap (official HotpotQA version).\"\"\"\n",
        "    normalized_prediction = normalize_answer(prediction)\n",
        "    normalized_ground_truth = normalize_answer(ground_truth)\n",
        "\n",
        "    ZERO_METRIC = (0.0, 0.0, 0.0) # F1, Precision, Recall\n",
        "\n",
        "    # Handling 'yes', 'no', 'noanswer' as non-overlapping tokenization causes issues.\n",
        "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
        "        return ZERO_METRIC\n",
        "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
        "        return ZERO_METRIC\n",
        "\n",
        "    prediction_tokens = normalized_prediction.split()\n",
        "    ground_truth_tokens = normalized_ground_truth.split()\n",
        "\n",
        "    # Token matching using Counter\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if num_same == 0:\n",
        "        return ZERO_METRIC\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1, precision, recall\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    \"\"\"Computes Exact Match score (official HotpotQA version).\"\"\"\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def compute_metrics(gold_answers, pred_answers):\n",
        "    \"\"\"Calculates Exact Match (EM), F1 Score, Precision, and Recall for a list of answers.\"\"\"\n",
        "    em_total = 0.0\n",
        "    f1_total = 0.0\n",
        "    prec_total = 0.0\n",
        "    recall_total = 0.0\n",
        "\n",
        "    for gold_list, pred in zip(gold_answers, pred_answers):\n",
        "        # HotpotQA official metric uses the MAX score across all gold answers\n",
        "        best_em = 0.0\n",
        "        best_f1 = 0.0\n",
        "        best_prec = 0.0\n",
        "        best_recall = 0.0\n",
        "\n",
        "        for gold_text in gold_list:\n",
        "            # 1. EM Score\n",
        "            em = exact_match_score(pred, gold_text)\n",
        "            best_em = max(best_em, float(em))\n",
        "\n",
        "            # 2. F1 Score (Returns F1, Precision, Recall tuple)\n",
        "            f1, prec, recall = f1_score(pred, gold_text)\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_prec = prec\n",
        "                best_recall = recall\n",
        "\n",
        "        em_total += best_em\n",
        "        f1_total += best_f1\n",
        "        prec_total += best_prec\n",
        "        recall_total += best_recall\n",
        "\n",
        "    num_samples = len(gold_answers)\n",
        "    return {\n",
        "        \"EM\": (em_total / num_samples) * 100,\n",
        "        \"F1\": (f1_total / num_samples) * 100,\n",
        "        \"Precision\": (prec_total / num_samples) * 100,\n",
        "        \"Recall\": (recall_total / num_samples) * 100,\n",
        "    }\n",
        "\n",
        "# NOTE: The previous compute_f1 function is removed as it's replaced by the official f1_score.\n",
        "# NOTE: All supporting fact (sp) and joint metrics logic is removed as this is a closed-book test.\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the HotpotQA evaluation.\"\"\"\n",
        "    tokenizer, model, device = load_model_and_tokenizer(MODEL_ID)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # 1. Load Dataset\n",
        "    print(f\"\\nLoading HotpotQA dataset (split: {SPLIT}, samples: {NUM_SAMPLES})...\")\n",
        "    try:\n",
        "        dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split=SPLIT)\n",
        "        # Filter for the first N samples\n",
        "        dataset = dataset.select(range(min(NUM_SAMPLES, len(dataset))))\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    gold_answers = []\n",
        "    pred_answers = []\n",
        "\n",
        "    print(\"\\nStarting closed-book evaluation...\")\n",
        "\n",
        "    for i, example in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "        # HotpotQA question format\n",
        "        question = example['question']\n",
        "\n",
        "        # The gold answer is a list of possible answers\n",
        "        # HotpotQA stores the answer as a string or list of strings.\n",
        "        # We ensure it's a list for consistency with multiple possible answers.\n",
        "        gold_answer_list = example['answer'] if isinstance(example['answer'], list) else [example['answer']]\n",
        "        gold_answers.append(gold_answer_list)\n",
        "\n",
        "        # --- Closed-Book Prompting ---\n",
        "        # The prompt only contains the question, not the context,\n",
        "        # forcing the model to rely on its internal knowledge.\n",
        "        # prompt = f\"Answer the following question concisely:\\nQuestion: {question}\\nAnswer:\"\n",
        "        prompt = f\"\"\"You are an expert at giving concise answers. Do not give any explanations, only a short answer. For example:\n",
        "        Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
        "        Answer: Arthur's Magazine\n",
        "\n",
        "        Question: Is Children's National Medical Center or MedStar Washington Hospital Center the largest private hospital in Washington, D.C.?\n",
        "        Answer: MedStar Washington Hospital Center\n",
        "\n",
        "        Now answer the question:\n",
        "\n",
        "        Question: {question}\n",
        "        Answer: \"\"\"\n",
        "\n",
        "        # Qwen-specific instruction format (if using the instruct version)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50, # Sufficient for a concise QA answer\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=tokenizer.eos_token_id # Important for generation\n",
        "            )\n",
        "\n",
        "        # Decode the generated text, skipping the input prompt\n",
        "        generated_text = tokenizer.decode(output[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        pred_answers.append(generated_text)\n",
        "\n",
        "        if i % (NUM_SAMPLES // 5 or 1) == 0 and i > 0:\n",
        "            print(f\"\\n--- Sample {i+1}/{len(dataset)} ---\")\n",
        "            print(f\"Q: {question}\")\n",
        "            # Print the first gold answer for readability\n",
        "            print(f\"Gold: {gold_answer_list[0]}\")\n",
        "            print(f\"Pred: {generated_text}\")\n",
        "\n",
        "    # 3. Compute Metrics\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "    metrics = compute_metrics(gold_answers, pred_answers)\n",
        "\n",
        "    print(f\"Results for {MODEL_ID} on HotpotQA ({len(dataset)} samples):\")\n",
        "    print(f\"  Exact Match (EM): {metrics['EM']:.2f}%\")\n",
        "    print(f\"  F1 Score (F1): {metrics['F1']:.2f}%\")\n",
        "    print(f\"  Precision: {metrics['Precision']:.2f}%\")\n",
        "    print(f\"  Recall: {metrics['Recall']:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure all required libraries are installed:\n",
        "    # pip install transformers datasets accelerate torch bitsandbytes tqdm\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "BBR-l8DshDkh",
      "metadata": {
        "id": "BBR-l8DshDkh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fsiddiqui/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-7B-Instruct...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Fetching 4 files: 100%|██████████| 4/4 [00:55<00:00, 13.96s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully on device: cpu\n",
            "\n",
            "Loading HotpotQA dataset (split: validation, samples: 10)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|██████████| 90447/90447 [00:02<00:00, 44690.37 examples/s]\n",
            "Generating validation split: 100%|██████████| 7405/7405 [00:00<00:00, 38545.71 examples/s]\n",
            "Generating test split: 100%|██████████| 7405/7405 [00:00<00:00, 45341.81 examples/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 10 samples.\n",
            "\n",
            "Starting closed-book evaluation...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/10 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            " 30%|███       | 3/10 [02:00<04:49, 41.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 3/10 ---\n",
            "Q: What science fantasy young adult series, told in first person, has a set of companion books narrating the stories of enslaved worlds and alien species?\n",
            "Gold: Animorphs\n",
            "Pred: Star Wars: The Rise of Kylo Ren\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [03:15<03:15, 39.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 5/10 ---\n",
            "Q: The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?\n",
            "Gold: Greenwich Village, New York City\n",
            "Pred: New York City\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [04:32<01:56, 38.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 7/10 ---\n",
            "Q: Who was known by his stage name Aladin and helped organizations improve their performance as a consultant?\n",
            "Gold: Eenasul Fateh\n",
            "Pred: Tom Peters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [05:47<00:37, 37.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Sample 9/10 ---\n",
            "Q: Who is older, Annie Morton or Terry Richardson?\n",
            "Gold: Terry Richardson\n",
            "Pred: Annie Morton\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [06:22<00:00, 38.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluation Complete ---\n",
            "Results for Qwen/Qwen2.5-7B-Instruct on HotpotQA (10 samples):\n",
            "  Exact Match (EM): 30.00%\n",
            "  F1 Score (F1): 37.50%\n",
            "  Precision: 40.00%\n",
            "  Recall: 36.00%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# --- Configuration ---\n",
        "# NOTE: Qwen2.5-7B-Instruct requires significant VRAM (>= 16GB) to run.\n",
        "# Consider using quantization (load_in_4bit=True) or a smaller model if resources are limited.\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DATASET_NAME = \"hotpot_qa\"\n",
        "SUBSET_NAME = \"fullwiki\"\n",
        "SPLIT = \"validation\" # Use the validation set for testing\n",
        "NUM_SAMPLES = 10    # Set a small number for quick testing (e.g., 100). Increase for full evaluation.\n",
        "\n",
        "def load_model_and_tokenizer(model_id):\n",
        "    \"\"\"Loads the model and tokenizer, optimizing for available hardware.\"\"\"\n",
        "    print(f\"Loading model: {model_id}...\")\n",
        "\n",
        "    # Use GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using Device: {device}\")\n",
        "\n",
        "    # Setup for model loading (using bfloat16 for better performance on modern GPUs)\n",
        "    kwargs = {\n",
        "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_properties(0).major >= 8 else torch.float16,\n",
        "        \"device_map\": \"auto\",\n",
        "        \"low_cpu_mem_usage\": True\n",
        "    }\n",
        "\n",
        "    # Try using 4-bit quantization if GPU is available to save VRAM\n",
        "    if device == \"cuda\" and torch.cuda.get_device_properties(0).total_memory / (1024**3) < 24:\n",
        "        print(\"Warning: Low VRAM detected. Attempting to load with 4-bit quantization.\")\n",
        "        try:\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
        "            kwargs[\"quantization_config\"] = bnb_config\n",
        "        except ImportError:\n",
        "            print(\"Install 'bitsandbytes' for 4-bit quantization to work on low VRAM.\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_id, **kwargs)\n",
        "        model.eval()\n",
        "        print(f\"Model loaded successfully on device: {device}\")\n",
        "        return tokenizer, model, device\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        print(\"Please ensure you have sufficient hardware (GPU/RAM) and required libraries installed.\")\n",
        "        return None, None, None\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"\n",
        "    Official HotpotQA normalization for answers.\n",
        "    Lowercases, removes punctuation, articles (a, an, the), and extra whitespace.\n",
        "    \"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    \"\"\"Computes F1 score based on token overlap (official HotpotQA version).\"\"\"\n",
        "    normalized_prediction = normalize_answer(prediction)\n",
        "    normalized_ground_truth = normalize_answer(ground_truth)\n",
        "\n",
        "    ZERO_METRIC = (0.0, 0.0, 0.0) # F1, Precision, Recall\n",
        "\n",
        "    # Handling 'yes', 'no', 'noanswer' as non-overlapping tokenization causes issues.\n",
        "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
        "        return ZERO_METRIC\n",
        "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
        "        return ZERO_METRIC\n",
        "\n",
        "    prediction_tokens = normalized_prediction.split()\n",
        "    ground_truth_tokens = normalized_ground_truth.split()\n",
        "\n",
        "    # Token matching using Counter\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if num_same == 0:\n",
        "        return ZERO_METRIC\n",
        "\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1, precision, recall\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    \"\"\"Computes Exact Match score (official HotpotQA version).\"\"\"\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "def compute_metrics(gold_answers, pred_answers):\n",
        "    \"\"\"Calculates Exact Match (EM), F1 Score, Precision, and Recall for a list of answers.\"\"\"\n",
        "    em_total = 0.0\n",
        "    f1_total = 0.0\n",
        "    prec_total = 0.0\n",
        "    recall_total = 0.0\n",
        "\n",
        "    for gold_list, pred in zip(gold_answers, pred_answers):\n",
        "        # HotpotQA official metric uses the MAX score across all gold answers\n",
        "        best_em = 0.0\n",
        "        best_f1 = 0.0\n",
        "        best_prec = 0.0\n",
        "        best_recall = 0.0\n",
        "\n",
        "        for gold_text in gold_list:\n",
        "            # 1. EM Score\n",
        "            em = exact_match_score(pred, gold_text)\n",
        "            best_em = max(best_em, float(em))\n",
        "\n",
        "            # 2. F1 Score (Returns F1, Precision, Recall tuple)\n",
        "            f1, prec, recall = f1_score(pred, gold_text)\n",
        "\n",
        "            if f1 > best_f1:\n",
        "                best_f1 = f1\n",
        "                best_prec = prec\n",
        "                best_recall = recall\n",
        "\n",
        "        em_total += best_em\n",
        "        f1_total += best_f1\n",
        "        prec_total += best_prec\n",
        "        recall_total += best_recall\n",
        "\n",
        "    num_samples = len(gold_answers)\n",
        "    return {\n",
        "        \"EM\": (em_total / num_samples) * 100,\n",
        "        \"F1\": (f1_total / num_samples) * 100,\n",
        "        \"Precision\": (prec_total / num_samples) * 100,\n",
        "        \"Recall\": (recall_total / num_samples) * 100,\n",
        "    }\n",
        "\n",
        "# NOTE: The previous compute_f1 function is removed as it's replaced by the official f1_score.\n",
        "# NOTE: All supporting fact (sp) and joint metrics logic is removed as this is a closed-book test.\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the HotpotQA evaluation.\"\"\"\n",
        "    tokenizer, model, device = load_model_and_tokenizer(MODEL_ID)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    # 1. Load Dataset\n",
        "    print(f\"\\nLoading HotpotQA dataset (split: {SPLIT}, samples: {NUM_SAMPLES})...\")\n",
        "    try:\n",
        "        dataset = load_dataset(DATASET_NAME, SUBSET_NAME, split=SPLIT)\n",
        "        # Filter for the first N samples\n",
        "        dataset = dataset.select(range(min(NUM_SAMPLES, len(dataset))))\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return\n",
        "\n",
        "    gold_answers = []\n",
        "    pred_answers = []\n",
        "\n",
        "    print(\"\\nStarting closed-book evaluation...\")\n",
        "\n",
        "    for i, example in tqdm(enumerate(dataset), total=len(dataset)):\n",
        "        # HotpotQA question format\n",
        "        question = example['question']\n",
        "\n",
        "        # The gold answer is a list of possible answers\n",
        "        # HotpotQA stores the answer as a string or list of strings.\n",
        "        # We ensure it's a list for consistency with multiple possible answers.\n",
        "        gold_answer_list = example['answer'] if isinstance(example['answer'], list) else [example['answer']]\n",
        "        gold_answers.append(gold_answer_list)\n",
        "\n",
        "        # --- Closed-Book Prompting ---\n",
        "        # The prompt only contains the question, not the context,\n",
        "        # forcing the model to rely on its internal knowledge.\n",
        "        # prompt = f\"Answer the following question concisely:\\nQuestion: {question}\\nAnswer:\"\n",
        "        prompt = f\"\"\"You are an expert at giving concise answers. Do not give any explanations, only a short answer. For example:\n",
        "        Question: Which magazine was started first Arthur's Magazine or First for Women?\n",
        "        Answer: Arthur's Magazine\n",
        "\n",
        "        Question: Is Children's National Medical Center or MedStar Washington Hospital Center the largest private hospital in Washington, D.C.?\n",
        "        Answer: MedStar Washington Hospital Center\n",
        "\n",
        "        Now answer the question:\n",
        "\n",
        "        Question: {question}\n",
        "        Answer: \"\"\"\n",
        "\n",
        "        # Qwen-specific instruction format (if using the instruct version)\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=50, # Sufficient for a concise QA answer\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                pad_token_id=tokenizer.eos_token_id # Important for generation\n",
        "            )\n",
        "\n",
        "        # Decode the generated text, skipping the input prompt\n",
        "        generated_text = tokenizer.decode(output[0, inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "        pred_answers.append(generated_text)\n",
        "\n",
        "        if i % (NUM_SAMPLES // 5 or 1) == 0 and i > 0:\n",
        "            print(f\"\\n--- Sample {i+1}/{len(dataset)} ---\")\n",
        "            print(f\"Q: {question}\")\n",
        "            # Print the first gold answer for readability\n",
        "            print(f\"Gold: {gold_answer_list[0]}\")\n",
        "            print(f\"Pred: {generated_text}\")\n",
        "\n",
        "    # 3. Compute Metrics\n",
        "    print(\"\\n--- Evaluation Complete ---\")\n",
        "    metrics = compute_metrics(gold_answers, pred_answers)\n",
        "\n",
        "    print(f\"Results for {MODEL_ID} on HotpotQA ({len(dataset)} samples):\")\n",
        "    print(f\"  Exact Match (EM): {metrics['EM']:.2f}%\")\n",
        "    print(f\"  F1 Score (F1): {metrics['F1']:.2f}%\")\n",
        "    print(f\"  Precision: {metrics['Precision']:.2f}%\")\n",
        "    print(f\"  Recall: {metrics['Recall']:.2f}%\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure all required libraries are installed:\n",
        "    # pip install transformers datasets accelerate torch bitsandbytes tqdm\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DZfcR0lzgsnz",
      "metadata": {
        "id": "DZfcR0lzgsnz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2.5-7B-Instruct...\n",
            "Using Device: cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.74s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 242\u001b[39m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[33m'\u001b[39m\u001b[33mRecall\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# Ensure all required libraries are installed:\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# pip install transformers datasets accelerate torch bitsandbytes tqdm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m    152\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Main function to run the HotpotQA evaluation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     tokenizer, model, device = \u001b[43mload_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mload_model_and_tokenizer\u001b[39m\u001b[34m(model_id)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     44\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     model.eval()\n\u001b[32m     47\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel loaded successfully on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:5048\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   5038\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5039\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   5041\u001b[39m     (\n\u001b[32m   5042\u001b[39m         model,\n\u001b[32m   5043\u001b[39m         missing_keys,\n\u001b[32m   5044\u001b[39m         unexpected_keys,\n\u001b[32m   5045\u001b[39m         mismatched_keys,\n\u001b[32m   5046\u001b[39m         offload_index,\n\u001b[32m   5047\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m5048\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5049\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5050\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5051\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5052\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5053\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5054\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5055\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5056\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5057\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5058\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5059\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5060\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5061\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5062\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5063\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5064\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5065\u001b[39m model.tie_weights()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:5468\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5465\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5467\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5468\u001b[39m         _error_msgs, disk_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5469\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5471\u001b[39m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:843\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     disk_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:750\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[39m\n\u001b[32m    748\u001b[39m param = param[...]\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    752\u001b[39m     param = param.contiguous()\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "56a46d5f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/fsiddiqui/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09610013",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "name": "fs2872 (Nov 16, 2025, 4:09:43 PM)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "047df1f9c5c749bbaa6e0496dacc494a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e1e960e5ef748e1af307b8891a1ca68",
            "placeholder": "​",
            "style": "IPY_MODEL_56ee725339e04140b2d8962ab9e211be",
            "value": " 4/4 [00:16&lt;00:00,  3.92s/it]"
          }
        },
        "07a57c7c68d44b36bb04ad81b5f102b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "260e11fef943412d8d3656a5716f897d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bfc86dd80f14d5b940f71e7c45c3e59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "325a2706c18945468fca290325026053": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bfc86dd80f14d5b940f71e7c45c3e59",
            "placeholder": "​",
            "style": "IPY_MODEL_c06096ac9fb14e19bbf98ece78e1bf9b",
            "value": " 4/4 [00:16&lt;00:00,  4.00s/it]"
          }
        },
        "3d8336c775eb4a79975ccced2ca0032d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f723cddcf74942c6b0bf8fc94bfaee6b",
              "IPY_MODEL_7577706f2fa0486f983f049ad5de5531",
              "IPY_MODEL_6fa76262f91a4078852787d48bbd003c"
            ],
            "layout": "IPY_MODEL_8e04976f083042d9b2b736abab09b75c"
          }
        },
        "4258b384016b413492ef322ba4df4ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2d49d848e4d4d6ea9b0dbc4dfa6577a",
              "IPY_MODEL_74355440b8b741d2942962c0e4b0d6f1",
              "IPY_MODEL_325a2706c18945468fca290325026053"
            ],
            "layout": "IPY_MODEL_44185fc3ee3b442aa4a958c54396638d"
          }
        },
        "44185fc3ee3b442aa4a958c54396638d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46200cb09c874a35a1f3e4c769d86c89": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56ee725339e04140b2d8962ab9e211be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5838ddd4ba514b16928be8fb51568029": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59d71ac2cba3451ab66caaa3ac250a99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ce7124a030a4c768b392e4b10b6ec58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60234a049e054bcbba47e586b30fd17d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ce7124a030a4c768b392e4b10b6ec58",
            "placeholder": "​",
            "style": "IPY_MODEL_8e31ca3154264784b3c9c2f9541dd691",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6fa76262f91a4078852787d48bbd003c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46200cb09c874a35a1f3e4c769d86c89",
            "placeholder": "​",
            "style": "IPY_MODEL_b42ffb2230bb408a957f39d9a8ad1d1a",
            "value": " 4/4 [00:16&lt;00:00,  3.90s/it]"
          }
        },
        "74355440b8b741d2942962c0e4b0d6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59d71ac2cba3451ab66caaa3ac250a99",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07a57c7c68d44b36bb04ad81b5f102b1",
            "value": 4
          }
        },
        "7577706f2fa0486f983f049ad5de5531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5838ddd4ba514b16928be8fb51568029",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_260e11fef943412d8d3656a5716f897d",
            "value": 4
          }
        },
        "8a15fe4fb0074e6cac55be9d48680d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_60234a049e054bcbba47e586b30fd17d",
              "IPY_MODEL_fedf1a7d07004549bd5c53af7526cf86",
              "IPY_MODEL_047df1f9c5c749bbaa6e0496dacc494a"
            ],
            "layout": "IPY_MODEL_b0f86f94c2044b4597ad938d8d87bcd1"
          }
        },
        "8e04976f083042d9b2b736abab09b75c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e31ca3154264784b3c9c2f9541dd691": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e1e960e5ef748e1af307b8891a1ca68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3f9b1b251f24884ada2abbf3700e965": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f86f94c2044b4597ad938d8d87bcd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2d49d848e4d4d6ea9b0dbc4dfa6577a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1c5e4cb8591429abe0581531ddd07f3",
            "placeholder": "​",
            "style": "IPY_MODEL_d85739665e9d492ea22b8baa12853da8",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b42ffb2230bb408a957f39d9a8ad1d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c06096ac9fb14e19bbf98ece78e1bf9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9ef8074f47a4772b695803a7170cd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d85739665e9d492ea22b8baa12853da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f1c5e4cb8591429abe0581531ddd07f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1f0d9e0be4e459f8bcb73b7498cce11": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f723cddcf74942c6b0bf8fc94bfaee6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3f9b1b251f24884ada2abbf3700e965",
            "placeholder": "​",
            "style": "IPY_MODEL_c9ef8074f47a4772b695803a7170cd0b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f7a4e0b529fc4ebb936a93407c8ffee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fedf1a7d07004549bd5c53af7526cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1f0d9e0be4e459f8bcb73b7498cce11",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7a4e0b529fc4ebb936a93407c8ffee9",
            "value": 4
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
